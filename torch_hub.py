#!/usr/bin/env python3
"""
æ”¯æŒçš„æ¨¡å‹ï¼š
- parseq_tiny: è½»é‡çº§æ¨¡å‹ (6M å‚æ•°)
- parseq: åŸºç¡€æ¨¡å‹ (23M å‚æ•°) 
- parseq_patch16_224: é«˜åˆ†è¾¨ç‡æ¨¡å‹
- abinet: ABINet æ¨¡å‹
- vitstr: ViTSTR æ¨¡å‹
- crnn: CRNN æ¨¡å‹
- trba: TRBA æ¨¡å‹
"""

import torch
from PIL import Image
import argparse
import os


def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼š")
    models = torch.hub.list('baudm/parseq', trust_repo=True)
    models = [m for m in models if not m.startswith('create_')]
    
    for model in models:
        print(f"  - {model}")
    return models


def load_model(model_name='parseq_tiny', device='cpu'):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    print(f"ğŸ“¥ åŠ è½½ {model_name} é¢„è®­ç»ƒæ¨¡å‹...")
    
    # åŠ è½½æ¨¡å‹
    model = torch.hub.load('baudm/parseq', model_name, pretrained=True, trust_repo=True)
    model = model.eval().to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"   æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   å‚æ•°é‡: {total_params:,}")
    
    return model


def recognize_text(model, image_path, device='cpu'):
    """ä½¿ç”¨æ¨¡å‹è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡æœ¬"""
    from torchvision import transforms
    
    # åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
    img = Image.open(image_path).convert('RGB')
    
    # PARSeq éœ€è¦ 128x32 çš„è¾“å…¥
    transform = transforms.Compose([
        transforms.Resize((32, 128)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    img_tensor = transform(img).unsqueeze(0).to(device)
    
    # è¿›è¡Œæ¨ç†
    with torch.no_grad():
        output = model(img_tensor)
    
    # è§£ç ç»“æœ (è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦ä½¿ç”¨æ¨¡å‹çš„ tokenizer)
    # å»ºè®®ä½¿ç”¨é¡¹ç›®æä¾›çš„ read.py è·å–å®Œæ•´åŠŸèƒ½
    return "éœ€è¦ä½¿ç”¨é¡¹ç›®çš„ read.py è„šæœ¬è·å–å®Œæ•´çš„æ–‡æœ¬è¯†åˆ«ç»“æœ"


def main():
    parser = argparse.ArgumentParser(description='PARSeq Torch Hub ç¤ºä¾‹')
    parser.add_argument('--model', default='parseq_tiny', 
                       help='æ¨¡å‹åç§° (é»˜è®¤: parseq_tiny)')
    parser.add_argument('--list-models', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹')
    parser.add_argument('--image', type=str,
                       help='è¦è¯†åˆ«çš„å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--device', default='cpu',
                       help='è®¾å¤‡ (cpu/cuda, é»˜è®¤: cpu)')
    
    args = parser.parse_args()
    
    if args.list_models:
        list_available_models()
        return
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.model, args.device)
    
    if args.image:
        if os.path.exists(args.image):
            print(f"\nğŸ” è¯†åˆ«å›¾ç‰‡: {args.image}")
            result = recognize_text(model, args.image, args.device)
            print(f"è¯†åˆ«ç»“æœ: {result}")
        else:
            print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
    
    print(f"\nğŸ’¡ è¦è·å¾—å®Œæ•´çš„æ–‡æœ¬è¯†åˆ«åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨:")
    print(f"python read.py pretrained={args.model.replace('_', '-')} --images <å›¾ç‰‡è·¯å¾„> --device {args.device}")


if __name__ == '__main__':
    main()
