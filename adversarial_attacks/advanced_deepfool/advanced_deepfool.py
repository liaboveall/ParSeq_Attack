#!/usr/bin/env python3
"""
SuperDeepFoolçš„ä¸»è¦æ”¹è¿›ï¼š
1. è‡ªé€‚åº”æ­¥é•¿æ§åˆ¶
2. å¤šç±»åˆ«å†³ç­–è¾¹ç•Œä¼˜åŒ–
3. æ¢¯åº¦åŠ¨é‡åŠ é€Ÿ
4. æ—©åœç­–ç•¥
5. åŠ¨æ€è¶…è°ƒå‚æ•°è°ƒæ•´
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
from pathlib import Path
import warnings
import sys
import time
from collections import deque

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (15, 8)
plt.rcParams['font.size'] = 12

warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")


def predict_text(model, images):
    """ä½¿ç”¨æ¨¡å‹çš„tokenizeræ­£ç¡®è§£ç æ–‡æœ¬"""
    with torch.no_grad():
        logits = model(images)
        probs = logits.softmax(-1)
        predictions, confidences = model.tokenizer.decode(probs)
    return predictions


class SuperDeepFoolAttacker:
    """SuperDeepFoolå¯¹æŠ—æ”»å‡»å™¨ - æ”¹è¿›ç‰ˆDeepFoolç®—æ³•"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.eval()
        self.device = device
        
        # SuperDeepFoolç‰¹æœ‰å‚æ•°
        self.momentum = 0.9
        self.adaptive_overshoot = True
        self.top_k_classes = 5
        self.convergence_threshold = 1e-6
        self.min_overshoot = 0.01
        self.max_overshoot = 0.1
        
    def superdeepfool_attack(self, images, max_iter=100, overshoot=0.02):
        """
        SuperDeepFoolæ”»å‡»å®ç°
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡ [B, C, H, W]
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            overshoot: åˆå§‹è¶…è°ƒå‚æ•°
            
        Returns:
            adversarial_images: å¯¹æŠ—æ ·æœ¬
            perturbations: æ‰°åŠ¨
            iterations: å®é™…è¿­ä»£æ¬¡æ•°
            convergence_history: æ”¶æ•›å†å²
        """
        images = images.clone().detach().to(self.device)
        batch_size = images.shape[0]
        
        adversarial_images = []
        perturbations = []
        iterations = []
        convergence_histories = []
        
        print(f"å¼€å§‹SuperDeepFoolæ”»å‡» (æ‰¹å¤§å°: {batch_size}, æœ€å¤§è¿­ä»£: {max_iter})...")
        
        for batch_idx in range(batch_size):
            print(f"  å¤„ç†å›¾åƒ {batch_idx + 1}/{batch_size}...")
            
            x = images[batch_idx:batch_idx+1].clone()
            r_total = torch.zeros_like(x)
            
            # è·å–åŸå§‹é¢„æµ‹
            with torch.no_grad():
                orig_logits = self.model(x)
                orig_pred_text = predict_text(self.model, x)[0]
            
            # SuperDeepFoolç‰¹æœ‰å˜é‡
            momentum_buffer = torch.zeros_like(x)
            current_overshoot = overshoot
            convergence_history = []
            patience_counter = 0
            best_perturbation = float('inf')
            no_improvement_count = 0
            
            start_time = time.time()
            
            for i in range(max_iter):
                x_adv = x + r_total
                x_adv.requires_grad_(True)
                
                # å‰å‘ä¼ æ’­
                logits = self.model(x_adv)
                
                # æ¯5è½®æ£€æŸ¥ä¸€æ¬¡é¢„æµ‹å˜åŒ–ï¼ˆå‡å°‘é¢‘ç¹æ£€æŸ¥ï¼‰
                if i % 5 == 0:
                    with torch.no_grad():
                        current_pred_text = predict_text(self.model, x_adv)[0]
                        if current_pred_text != orig_pred_text:
                            iterations.append(i + 1)
                            break
                
                # è®¡ç®—å¤šç±»åˆ«æ¢¯åº¦
                grad_dict = self._compute_multiclass_gradients(x_adv, logits)
                
                # é€‰æ‹©æœ€ä¼˜æ‰°åŠ¨æ–¹å‘
                optimal_perturbation = self._select_optimal_perturbation(grad_dict, current_overshoot)
                
                # åº”ç”¨åŠ¨é‡
                momentum_buffer = self.momentum * momentum_buffer + (1 - self.momentum) * optimal_perturbation
                
                # æ›´æ–°æ‰°åŠ¨
                r_total = r_total + momentum_buffer
                
                # è®¡ç®—å½“å‰æ‰°åŠ¨å¤§å°
                current_norm = torch.norm(r_total).item()
                convergence_history.append(current_norm)
                
                # è‡ªé€‚åº”è°ƒæ•´è¶…è°ƒå‚æ•°
                if self.adaptive_overshoot:
                    current_overshoot = self._adaptive_overshoot_update(
                        current_norm, best_perturbation, current_overshoot, i
                    )
                
                # æ—©åœæ£€æŸ¥ï¼ˆå‡å°‘æ£€æŸ¥é¢‘ç‡ï¼‰
                if i > 10 and i % 5 == 0 and self._check_convergence(convergence_history):
                    print(f"    æ”¶æ•›æ£€æµ‹: ç¬¬ {i+1} è½®æå‰åœæ­¢")
                    iterations.append(i + 1)
                    break
                
                # æ›´æ–°æœ€ä½³æ‰°åŠ¨
                if current_norm < best_perturbation:
                    best_perturbation = current_norm
                    no_improvement_count = 0
                else:
                    no_improvement_count += 1
                
                # è€å¿ƒæœºåˆ¶
                if no_improvement_count > 10:
                    current_overshoot = min(current_overshoot * 1.1, self.max_overshoot)
                    no_improvement_count = 0
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 20 == 0:
                    elapsed = time.time() - start_time
                    print(f"    è½®æ¬¡ {i+1:3d}: æ‰°åŠ¨L2={current_norm:.6f}, "
                          f"è¶…è°ƒ={current_overshoot:.4f}, è€—æ—¶={elapsed:.1f}s")
            else:
                # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                iterations.append(max_iter)
            
            # ç”Ÿæˆæœ€ç»ˆå¯¹æŠ—æ ·æœ¬
            final_adv = torch.clamp(x + r_total, 0, 1)
            adversarial_images.append(final_adv.squeeze(0))
            perturbations.append(r_total.squeeze(0))
            convergence_histories.append(convergence_history)
            
            elapsed = time.time() - start_time
            print(f"    å®Œæˆ (è¿­ä»£: {iterations[-1]}, æœ€ç»ˆL2èŒƒæ•°: {torch.norm(r_total).item():.6f}, "
                  f"æ€»è€—æ—¶: {elapsed:.1f}s)")
        
        adversarial_images = torch.stack(adversarial_images)
        perturbations = torch.stack(perturbations)
        
        print("SuperDeepFoolæ”»å‡»å®Œæˆ!")
        return adversarial_images.detach(), perturbations.detach(), iterations, convergence_histories
    
    def _compute_multiclass_gradients(self, x_adv, logits):
        """è®¡ç®—å¤šç±»åˆ«æ¢¯åº¦ - ä¼˜åŒ–ç‰ˆæœ¬"""
        seq_len, vocab_size = logits.shape[1], logits.shape[2]
        grad_dict = {}
        
        # è·å–å½“å‰é¢„æµ‹çš„top-kç±»åˆ«ï¼ˆå‡å°‘kå€¼ä»¥æé«˜é€Ÿåº¦ï¼‰
        current_logits = logits.view(-1, vocab_size)
        topk_k = min(3, self.top_k_classes)  # é™åˆ¶ä¸ºæœ€å¤š3ä¸ªç±»åˆ«
        topk_values, topk_indices = torch.topk(current_logits, topk_k, dim=-1)
        
        # åªå¤„ç†å‰å‡ ä¸ªæ—¶é—´æ­¥ä»¥æé«˜é€Ÿåº¦
        max_time_steps = min(seq_len, 8)  # é™åˆ¶æ—¶é—´æ­¥æ•°
        
        for t in range(max_time_steps):
            current_class = topk_indices[t, 0]
            
            # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰éœ€è¦çš„æ¢¯åº¦
            self.model.zero_grad()
            if x_adv.grad is not None:
                x_adv.grad.zero_()
                
            # è®¡ç®—å½“å‰ç±»åˆ«çš„æŸå¤±
            loss_current = current_logits[t, current_class]
            loss_current.backward(retain_graph=True)
            grad_current = x_adv.grad.clone() if x_adv.grad is not None else torch.zeros_like(x_adv)
            
            # åªå¤„ç†æœ€é‡è¦çš„1-2ä¸ªå…¶ä»–ç±»åˆ«
            for k in range(1, min(2, topk_k)):  # å‡å°‘å¤„ç†çš„ç±»åˆ«æ•°
                if k < topk_indices.shape[1]:
                    other_class = topk_indices[t, k]
                    
                    self.model.zero_grad()
                    if x_adv.grad is not None:
                        x_adv.grad.zero_()
                    
                    loss_other = current_logits[t, other_class]
                    loss_other.backward(retain_graph=True)
                    grad_other = x_adv.grad.clone() if x_adv.grad is not None else torch.zeros_like(x_adv)
                    
                    # å­˜å‚¨æ¢¯åº¦å·®å’Œå†³ç­–è¾¹ç•Œè·ç¦»
                    grad_diff = grad_other - grad_current
                    decision_boundary_dist = (loss_other - loss_current).item()
                    
                    key = f"t{t}_k{k}"
                    grad_dict[key] = {
                        'grad_diff': grad_diff,
                        'distance': abs(decision_boundary_dist),
                        'logit_diff': decision_boundary_dist
                    }
        
        return grad_dict
    
    def _select_optimal_perturbation(self, grad_dict, overshoot):
        """é€‰æ‹©æœ€ä¼˜æ‰°åŠ¨æ–¹å‘"""
        if not grad_dict:
            return torch.zeros_like(list(grad_dict.values())[0]['grad_diff'])
        
        min_perturbation_norm = float('inf')
        optimal_perturbation = None
        
        for key, info in grad_dict.items():
            grad_diff = info['grad_diff']
            distance = info['distance']
            
            # è®¡ç®—æ‰°åŠ¨æ–¹å‘çš„L2èŒƒæ•°
            grad_norm = torch.norm(grad_diff.flatten()).item()
            
            if grad_norm > 1e-8 and distance > 1e-8:
                # è®¡ç®—æœ€å°æ‰°åŠ¨
                perturbation = (distance / (grad_norm ** 2)) * grad_diff
                perturbation_norm = torch.norm(perturbation.flatten()).item()
                
                if perturbation_norm < min_perturbation_norm:
                    min_perturbation_norm = perturbation_norm
                    optimal_perturbation = perturbation
        
        if optimal_perturbation is not None:
            return (1 + overshoot) * optimal_perturbation
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ‰°åŠ¨ï¼Œä½¿ç”¨éšæœºæ‰°åŠ¨
            return 0.001 * torch.randn_like(list(grad_dict.values())[0]['grad_diff'])
    
    def _adaptive_overshoot_update(self, current_norm, best_norm, current_overshoot, iteration):
        """è‡ªé€‚åº”æ›´æ–°è¶…è°ƒå‚æ•°"""
        if iteration < 5:
            return current_overshoot
        
        # å¦‚æœæ‰°åŠ¨åœ¨å‡å°ï¼Œå‡å°‘è¶…è°ƒ
        if current_norm < best_norm:
            new_overshoot = max(current_overshoot * 0.95, self.min_overshoot)
        else:
            # å¦‚æœæ‰°åŠ¨åœ¨å¢å¤§ï¼Œå¢åŠ è¶…è°ƒ
            new_overshoot = min(current_overshoot * 1.05, self.max_overshoot)
        
        return new_overshoot
    
    def _check_convergence(self, history, window_size=5):
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›"""
        if len(history) < window_size * 2:
            return False
        
        recent_values = history[-window_size:]
        previous_values = history[-window_size*2:-window_size]
        
        recent_mean = np.mean(recent_values)
        previous_mean = np.mean(previous_values)
        
        # å¦‚æœæœ€è¿‘çš„å˜åŒ–å¾ˆå°ï¼Œè®¤ä¸ºæ”¶æ•›
        relative_change = abs(recent_mean - previous_mean) / (previous_mean + 1e-8)
        return relative_change < self.convergence_threshold


def select_model():
    """äº¤äº’å¼æ¨¡å‹é€‰æ‹©"""
    models = {
        1: 'parseq_tiny',
        2: 'parseq_patch16_224', 
        3: 'parseq',
        4: 'parseq_base'
    }
    
    descriptions = {
        1: 'å¾®å‹æ¨¡å‹ - æœ€å¿«é€Ÿåº¦ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•',
        2: 'æ ‡å‡†æ¨¡å‹ - å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦',
        3: 'å®Œæ•´æ¨¡å‹ - æœ€ä½³æ€§èƒ½', 
        4: 'åŸºç¡€æ¨¡å‹ - æ ‡å‡†é…ç½®'
    }
    
    print("\nè¯·é€‰æ‹©PARSeqæ¨¡å‹:")
    for key, desc in descriptions.items():
        print(f"  {key}. {models[key]} - {desc}")
    
    while True:
        try:
            choice = int(input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): "))
            if choice in models:
                model_name = models[choice]
                print(f"âœ“ å·²é€‰æ‹©: {model_name}")
                return model_name
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-4ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")


def select_dataset_scope():
    """é€‰æ‹©æ•°æ®é›†èŒƒå›´"""
    scopes = {
        1: (5, "å°æ ·æœ¬æµ‹è¯• - 5å¼ å›¾åƒ"),
        2: (15, "ä¸­ç­‰è§„æ¨¡æµ‹è¯• - 15å¼ å›¾åƒ"), 
        3: (30, "å¤§è§„æ¨¡æµ‹è¯• - 30å¼ å›¾åƒ"),
        4: (80, "å®Œæ•´æ•°æ®é›† - 80å¼ å›¾åƒ")
    }
    
    print("\nè¯·é€‰æ‹©æµ‹è¯•èŒƒå›´:")
    for key, (num, desc) in scopes.items():
        print(f"  {key}. {desc}")
    
    while True:
        try:
            choice = int(input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): "))
            if choice in scopes:
                num_images, desc = scopes[choice]
                print(f"âœ“ å·²é€‰æ‹©: {desc}")
                return num_images
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-4ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")


def load_cute80_images(num_images=5):
    """åŠ è½½CUTE80æ•°æ®é›†å›¾åƒ"""
    cute80_dir = Path(__file__).parent.parent.parent / "CUTE80"
    
    if not cute80_dir.exists():
        raise FileNotFoundError(f"CUTE80æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {cute80_dir}")
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(list(cute80_dir.glob(f"*{ext}")))
    
    if len(image_files) == 0:
        raise FileNotFoundError(f"åœ¨ {cute80_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
    
    # æ’åºå¹¶é€‰æ‹©æŒ‡å®šæ•°é‡çš„å›¾åƒ
    image_files.sort()
    selected_files = image_files[:num_images]
    
    print(f"\nä»CUTE80æ•°æ®é›†åŠ è½½ {len(selected_files)} å¼ å›¾åƒ:")
    for i, img_path in enumerate(selected_files):
        print(f"  {i+1}. {img_path.name}")
    
    return selected_files


def preprocess_images(image_files, model):
    """é¢„å¤„ç†å›¾åƒ"""
    from strhub.data.module import SceneTextDataModule
    transform = SceneTextDataModule.get_transform(model.hparams.img_size)
    
    original_images = []
    image_tensors = []
    image_names = []
    
    print("\næ­£åœ¨é¢„å¤„ç†å›¾åƒ...")
    for i, img_path in enumerate(image_files):
        try:
            # åŠ è½½åŸå§‹å›¾åƒ
            orig_img = Image.open(img_path).convert('RGB')
            original_images.append(orig_img)
            
            # é¢„å¤„ç†
            img_tensor = transform(orig_img)
            image_tensors.append(img_tensor)
            
            # è·å–å›¾åƒå
            image_names.append(img_path.stem)
            
            print(f"  {i+1}. {img_path.name} -> {orig_img.size}")
            
        except Exception as e:
            print(f"  è­¦å‘Š: åŠ è½½å›¾åƒ {img_path.name} å¤±è´¥: {e}")
            continue
    
    if len(image_tensors) == 0:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒï¼")
    
    # è½¬æ¢ä¸ºbatch
    images_batch = torch.stack(image_tensors).to(device)
    print(f"\nå›¾åƒæ‰¹æ¬¡å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {images_batch.shape}")
    
    return original_images, images_batch, image_names


def visualize_results(original_images, original_texts, adversarial_texts, 
                     perturbations, image_names, convergence_histories, save_dir=None):
    """å¯è§†åŒ–æ”»å‡»ç»“æœå’Œæ”¶æ•›æ›²çº¿"""
    num_images = len(original_images)
    
    # åˆ›å»ºå­å›¾å¸ƒå±€ï¼šåŸå§‹å›¾åƒã€æ‰°åŠ¨ã€æ”¶æ•›æ›²çº¿
    fig = plt.figure(figsize=(20, 6 * ((num_images + 2) // 3)))
    
    rows = (num_images + 2) // 3
    cols = 3
    
    # ä¸»æ ‡é¢˜
    fig.suptitle("SuperDeepFoolæ”»å‡»ç»“æœå¯¹æ¯”", fontsize=20, fontweight='bold')
    
    for i in range(num_images):
        row = i // cols
        col = i % cols
        
        # åˆ›å»ºå­å›¾ç½‘æ ¼
        ax1 = plt.subplot2grid((rows * 3, cols), (row * 3, col))
        ax2 = plt.subplot2grid((rows * 3, cols), (row * 3 + 1, col))
        ax3 = plt.subplot2grid((rows * 3, cols), (row * 3 + 2, col))
        
        # åŸå§‹å›¾åƒ
        ax1.imshow(original_images[i])
        ax1.set_title(f'åŸå§‹å›¾åƒ {i+1}\nè¯†åˆ«: "{original_texts[i]}"', fontsize=10)
        ax1.axis('off')
        
        # æ‰°åŠ¨å¯è§†åŒ–
        pert = perturbations[i].cpu().numpy()
        pert_vis = np.transpose(pert, (1, 2, 0))
        pert_vis = (pert_vis - pert_vis.min()) / (pert_vis.max() - pert_vis.min() + 1e-8)
        
        ax2.imshow(pert_vis)
        attack_status = "æˆåŠŸ" if original_texts[i] != adversarial_texts[i] else "å¤±è´¥"
        ax2.set_title(f'æ‰°åŠ¨å¯è§†åŒ–\næ”»å‡»å: "{adversarial_texts[i]}"\nçŠ¶æ€: {attack_status}', fontsize=10)
        ax2.axis('off')
        
        # æ”¶æ•›æ›²çº¿
        if i < len(convergence_histories) and convergence_histories[i]:
            ax3.plot(convergence_histories[i], 'b-', linewidth=2)
            ax3.set_title(f'æ”¶æ•›æ›²çº¿\næœ€ç»ˆL2èŒƒæ•°: {convergence_histories[i][-1]:.4f}', fontsize=10)
            ax3.set_xlabel('è¿­ä»£æ¬¡æ•°')
            ax3.set_ylabel('æ‰°åŠ¨L2èŒƒæ•°')
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'æ— æ”¶æ•›æ•°æ®', ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('æ”¶æ•›æ›²çº¿', fontsize=10)
    
    plt.tight_layout()
    
    if save_dir:
        save_path = Path(save_dir) / "superdeepfool_results.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç»“æœå›¾åƒå·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å¼"""
    print("ğŸš€ SuperDeepFoolå¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("=" * 50)
    
    # å›ºå®šé…ç½®ä»¥æé«˜é€Ÿåº¦
    config = {
        'model_name': 'parseq_tiny',
        'num_samples': 3,
        'max_iter': 10,
        'top_k_classes': 2,
        'overshoot': 0.05
    }
    
    print(f"æ¨¡å‹: {config['model_name']}")
    print(f"æ ·æœ¬æ•°: {config['num_samples']}")
    print(f"æœ€å¤§è¿­ä»£: {config['max_iter']}")
    print(f"Top-Kç±»åˆ«: {config['top_k_classes']}")
    
    try:
        # åŠ è½½æ¨¡å‹
        print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
        start_time = time.time()
        model = load_model(config['model_name'])
        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({time.time() - start_time:.1f}s)")
        
        # åŠ è½½æ•°æ®
        print("\nğŸ“ åŠ è½½æµ‹è¯•æ•°æ®...")
        start_time = time.time()
        data_loader = load_cute80_data(num_samples=config['num_samples'])
        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ ({time.time() - start_time:.1f}s)")
        
        # åˆå§‹åŒ–æ”»å‡»å™¨
        print("\nâš¡ åˆå§‹åŒ–SuperDeepFoolæ”»å‡»å™¨...")
        attacker = SuperDeepFoolAttacker(
            model=model,
            top_k_classes=config['top_k_classes'],
            momentum=0.8,
            adaptive_overshoot=True
        )
        
        # æ‰§è¡Œæ”»å‡»
        print(f"\nğŸ¯ å¼€å§‹æ”»å‡»æµ‹è¯•...")
        attack_start = time.time()
        
        for batch_images, batch_labels in data_loader:
            adversarial_images, perturbations, iterations, histories = attacker.attack(
                batch_images, 
                max_iter=config['max_iter'],
                overshoot=config['overshoot']
            )
            
            # è®¡ç®—æ”»å‡»ç»“æœ
            success_rate = calculate_attack_success_rate(
                model, batch_images, adversarial_images, batch_labels
            )
            
            total_time = time.time() - attack_start
            
            print(f"\nğŸ“Š å¿«é€Ÿæµ‹è¯•ç»“æœ:")
            print(f"  â€¢ å¤„ç†å›¾åƒ: {len(batch_images)}")
            print(f"  â€¢ æ”»å‡»æˆåŠŸç‡: {success_rate:.1%}")
            print(f"  â€¢ å¹³å‡è¿­ä»£æ¬¡æ•°: {np.mean(iterations):.1f}")
            print(f"  â€¢ æ€»è€—æ—¶: {total_time:.1f}s")
            print(f"  â€¢ å¹³å‡æ¯å¼ å›¾åƒ: {total_time/len(batch_images):.1f}s")
            
            break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            
        print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–é¡¹")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("SuperDeepFoolå¯¹æŠ—æ”»å‡» - æ”¹è¿›ç‰ˆDeepFoolç®—æ³•")
    print("é€‚ç”¨äºPARSeqæ–‡æœ¬è¯†åˆ«æ¨¡å‹")
    print("=" * 80)
    
    try:
        # 1. é€‰æ‹©æ¨¡å‹
        model_name = select_model()
        
        # 2. åŠ è½½æ¨¡å‹
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}...")
        model = torch.hub.load('baudm/parseq', model_name, pretrained=True, trust_repo=True)
        model.eval()
        model.to(device)
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"  å‚æ•°é‡: {total_params:,}")
        print(f"  è¾“å…¥å°ºå¯¸: {model.hparams.img_size}")
        
        # 3. é€‰æ‹©æ•°æ®é›†èŒƒå›´
        num_images = select_dataset_scope()
        
        # 4. åŠ è½½å›¾åƒ
        image_files = load_cute80_images(num_images)
        original_images, images_batch, image_names = preprocess_images(image_files, model)
        
        # 5. åŸå§‹é¢„æµ‹
        print("\næ­£åœ¨è¿›è¡ŒåŸå§‹æ–‡æœ¬è¯†åˆ«...")
        original_texts = predict_text(model, images_batch)
        
        print("\nåŸå§‹è¯†åˆ«ç»“æœ:")
        for i, (name, text) in enumerate(zip(image_names, original_texts)):
            print(f"  {i+1}. {name}: \"{text}\"")
        
        # 6. åˆ›å»ºSuperDeepFoolæ”»å‡»å™¨
        attacker = SuperDeepFoolAttacker(model, device)
        
        # 7. æ‰§è¡ŒSuperDeepFoolæ”»å‡»
        print(f"\nå¼€å§‹æ‰§è¡ŒSuperDeepFoolæ”»å‡»...")
        max_iter = 100
        overshoot = 0.02
        
        start_time = time.time()
        adversarial_images, perturbations, iterations, convergence_histories = attacker.superdeepfool_attack(
            images_batch, max_iter=max_iter, overshoot=overshoot
        )
        total_time = time.time() - start_time
        
        # 8. æ”»å‡»åé¢„æµ‹
        print("\næ­£åœ¨è¯†åˆ«å¯¹æŠ—æ ·æœ¬...")
        adversarial_texts = predict_text(model, adversarial_images)
        
        # 9. åˆ†æç»“æœ
        print("\n" + "="*80)
        print("SuperDeepFoolæ”»å‡»ç»“æœåˆ†æ")
        print("="*80)
        
        success_count = 0
        total_perturbation = 0
        
        for i in range(len(original_texts)):
            attack_success = original_texts[i] != adversarial_texts[i]
            if attack_success:
                success_count += 1
            
            pert_norm = torch.norm(perturbations[i]).item()
            total_perturbation += pert_norm
            
            status = "âœ“ æˆåŠŸ" if attack_success else "âœ— å¤±è´¥"
            print(f"{i+1}. {image_names[i]}:")
            print(f"   åŸå§‹: \"{original_texts[i]}\"")
            print(f"   æ”»å‡»å: \"{adversarial_texts[i]}\"")
            print(f"   çŠ¶æ€: {status}")
            print(f"   è¿­ä»£æ¬¡æ•°: {iterations[i]}")
            print(f"   æ‰°åŠ¨L2èŒƒæ•°: {pert_norm:.6f}")
            print("-" * 60)
        
        success_rate = success_count / len(original_texts)
        avg_perturbation = total_perturbation / len(original_texts)
        avg_iterations = sum(iterations) / len(iterations)
        
        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print(f"  æ”»å‡»æˆåŠŸç‡: {success_count}/{len(original_texts)} = {success_rate:.1%}")
        print(f"  å¹³å‡æ‰°åŠ¨L2èŒƒæ•°: {avg_perturbation:.6f}")
        print(f"  å¹³å‡è¿­ä»£æ¬¡æ•°: {avg_iterations:.1f}")
        print(f"  æ€»æ”»å‡»æ—¶é—´: {total_time:.1f}ç§’")
        print(f"  å¹³å‡æ¯å¼ å›¾åƒ: {total_time/len(original_texts):.1f}ç§’")
        
        # 10. ä¿å­˜ç»“æœ
        save_dir = Path(__file__).parent / "results"
        save_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_file = save_dir / f"superdeepfool_stats_{model_name}.txt"
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("SuperDeepFoolæ”»å‡»ç»Ÿè®¡ç»“æœ\n")
            f.write("="*50 + "\n")
            f.write(f"æ¨¡å‹: {model_name}\n")
            f.write(f"æµ‹è¯•å›¾åƒæ•°: {len(original_texts)}\n")
            f.write(f"æ”»å‡»æˆåŠŸç‡: {success_rate:.1%}\n")
            f.write(f"å¹³å‡æ‰°åŠ¨L2èŒƒæ•°: {avg_perturbation:.6f}\n")
            f.write(f"å¹³å‡è¿­ä»£æ¬¡æ•°: {avg_iterations:.1f}\n")
            f.write(f"æ€»æ”»å‡»æ—¶é—´: {total_time:.1f}ç§’\n")
            f.write(f"å¹³å‡æ¯å¼ å›¾åƒ: {total_time/len(original_texts):.1f}ç§’\n\n")
            
            for i in range(len(original_texts)):
                attack_success = original_texts[i] != adversarial_texts[i]
                status = "æˆåŠŸ" if attack_success else "å¤±è´¥"
                pert_norm = torch.norm(perturbations[i]).item()
                
                f.write(f"{i+1}. {image_names[i]}:\n")
                f.write(f"   åŸå§‹: \"{original_texts[i]}\"\n")
                f.write(f"   æ”»å‡»å: \"{adversarial_texts[i]}\"\n")
                f.write(f"   çŠ¶æ€: {status}\n")
                f.write(f"   è¿­ä»£æ¬¡æ•°: {iterations[i]}\n")
                f.write(f"   æ‰°åŠ¨L2èŒƒæ•°: {pert_norm:.6f}\n")
                f.write("-" * 30 + "\n")
        
        print(f"\nè¯¦ç»†ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_file}")
        
        # 11. å¯è§†åŒ–ç»“æœ
        print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        visualize_results(original_images, original_texts, adversarial_texts, 
                         perturbations, image_names, convergence_histories, save_dir)
        
        print("\n" + "="*80)
        print("SuperDeepFoolæ”»å‡»å®éªŒå®Œæˆï¼")
        print("="*80)
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("SuperDeepFoolæ”»å‡»è„šæœ¬")
    print("1. å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("2. å®Œæ•´æ”»å‡»æ¨¡å¼")
    
    choice = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1/2, é»˜è®¤1): ").strip()
    
    if choice == "2":
        main()
    else:
        quick_test()
